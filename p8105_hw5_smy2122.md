Homework 5
================
Sarah Younes

I set “scipen” = 1000 to prevent scientific notation (especially for
small p-values in Problem 3) and always see the full number.

As always, I will begin by loading the packages I will need for this
assignment: tidyverse. Then, I will `set.seed` so that I get the same
values in Problem 3 when I generate data sets and enhance
reproducibility. Additionally, I will set my theme and legend position
for my plots.

``` r
library(tidyverse)
set.seed(1)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options("scipen" = 1000)
```

## Problem 1

First, I will import the data.

``` r
homicide_data =
  read.csv("./data/homicide-data.csv")
```

Next, I will clean the data.

``` r
homicide_data =
  homicide_data |>
  janitor::clean_names() |>
  mutate(
    victim_last = str_to_title(victim_last),
    victim_first = str_to_title(victim_first)) |>
  mutate(city_state = paste(city, state, sep = ", "))
```

Now, I will summarize within cities to obtain the total number of
homicides and the total number of unsolved homicides, and I will show
both of these values for each city.

``` r
total_homicides =
  homicide_data |>
  group_by(city) |>
  summarize(
    total_homicides = n())

unresolved_homicides =
  homicide_data |>
  filter(disposition == "Closed without arrest" | disposition == "Open/No arrest") |>
  group_by(city) |>
  summarize(
    unresolved_homicides = n())

city_summaries =
  full_join(total_homicides, unresolved_homicides, by = "city") |>
  print(n = Inf)
```

    ## # A tibble: 50 × 3
    ##    city           total_homicides unresolved_homicides
    ##    <chr>                    <int>                <int>
    ##  1 Albuquerque                378                  146
    ##  2 Atlanta                    973                  373
    ##  3 Baltimore                 2827                 1825
    ##  4 Baton Rouge                424                  196
    ##  5 Birmingham                 800                  347
    ##  6 Boston                     614                  310
    ##  7 Buffalo                    521                  319
    ##  8 Charlotte                  687                  206
    ##  9 Chicago                   5535                 4073
    ## 10 Cincinnati                 694                  309
    ## 11 Columbus                  1084                  575
    ## 12 Dallas                    1567                  754
    ## 13 Denver                     312                  169
    ## 14 Detroit                   2519                 1482
    ## 15 Durham                     276                  101
    ## 16 Fort Worth                 549                  255
    ## 17 Fresno                     487                  169
    ## 18 Houston                   2942                 1493
    ## 19 Indianapolis              1322                  594
    ## 20 Jacksonville              1168                  597
    ## 21 Kansas City               1190                  486
    ## 22 Las Vegas                 1381                  572
    ## 23 Long Beach                 378                  156
    ## 24 Los Angeles               2257                 1106
    ## 25 Louisville                 576                  261
    ## 26 Memphis                   1514                  483
    ## 27 Miami                      744                  450
    ## 28 Milwaukee                 1115                  403
    ## 29 Minneapolis                366                  187
    ## 30 Nashville                  767                  278
    ## 31 New Orleans               1434                  930
    ## 32 New York                   627                  243
    ## 33 Oakland                    947                  508
    ## 34 Oklahoma City              672                  326
    ## 35 Omaha                      409                  169
    ## 36 Philadelphia              3037                 1360
    ## 37 Phoenix                    914                  504
    ## 38 Pittsburgh                 631                  337
    ## 39 Richmond                   429                  113
    ## 40 Sacramento                 376                  139
    ## 41 San Antonio                833                  357
    ## 42 San Bernardino             275                  170
    ## 43 San Diego                  461                  175
    ## 44 San Francisco              663                  336
    ## 45 Savannah                   246                  115
    ## 46 St. Louis                 1677                  905
    ## 47 Stockton                   444                  266
    ## 48 Tampa                      208                   95
    ## 49 Tulsa                      584                  193
    ## 50 Washington                1345                  589

## Problem 2

First, I will start with a data frame containing all file names.

``` r
csv_files =
  list.files(path = "./data/problem_2", pattern = "\\.csv$", full.names = TRUE)

csv_files_df =
  csv_files |>
  tibble()
```

First, I will create a data frame containing all file names via
`list.files` and paths. Then, I will use `purrr::map` to import each csv
file into the data frame, and all their data is saved as a new `data`
variable in the data frame.

``` r
df =
  tibble(
    files = list.files("./data/problem_2/", pattern = "\\.csv$"),
    path = str_c("./data/problem_2/", files)) |>
  mutate(
    data = map(path, read_csv))

df
```

    ## # A tibble: 20 × 3
    ##    files      path                        data              
    ##    <chr>      <chr>                       <list>            
    ##  1 con_01.csv ./data/problem_2/con_01.csv <spc_tbl_ [1 × 8]>
    ##  2 con_02.csv ./data/problem_2/con_02.csv <spc_tbl_ [1 × 8]>
    ##  3 con_03.csv ./data/problem_2/con_03.csv <spc_tbl_ [1 × 8]>
    ##  4 con_04.csv ./data/problem_2/con_04.csv <spc_tbl_ [1 × 8]>
    ##  5 con_05.csv ./data/problem_2/con_05.csv <spc_tbl_ [1 × 8]>
    ##  6 con_06.csv ./data/problem_2/con_06.csv <spc_tbl_ [1 × 8]>
    ##  7 con_07.csv ./data/problem_2/con_07.csv <spc_tbl_ [1 × 8]>
    ##  8 con_08.csv ./data/problem_2/con_08.csv <spc_tbl_ [1 × 8]>
    ##  9 con_09.csv ./data/problem_2/con_09.csv <spc_tbl_ [1 × 8]>
    ## 10 con_10.csv ./data/problem_2/con_10.csv <spc_tbl_ [1 × 8]>
    ## 11 exp_01.csv ./data/problem_2/exp_01.csv <spc_tbl_ [1 × 8]>
    ## 12 exp_02.csv ./data/problem_2/exp_02.csv <spc_tbl_ [1 × 8]>
    ## 13 exp_03.csv ./data/problem_2/exp_03.csv <spc_tbl_ [1 × 8]>
    ## 14 exp_04.csv ./data/problem_2/exp_04.csv <spc_tbl_ [1 × 8]>
    ## 15 exp_05.csv ./data/problem_2/exp_05.csv <spc_tbl_ [1 × 8]>
    ## 16 exp_06.csv ./data/problem_2/exp_06.csv <spc_tbl_ [1 × 8]>
    ## 17 exp_07.csv ./data/problem_2/exp_07.csv <spc_tbl_ [1 × 8]>
    ## 18 exp_08.csv ./data/problem_2/exp_08.csv <spc_tbl_ [1 × 8]>
    ## 19 exp_09.csv ./data/problem_2/exp_09.csv <spc_tbl_ [1 × 8]>
    ## 20 exp_10.csv ./data/problem_2/exp_10.csv <spc_tbl_ [1 × 8]>

Next, I will clean the data to unnest all the weekly data from the
`data` variable, create ID and study arm variables,tidy the week
variable, set `week` as a numeric value, and select the variables I
need.

``` r
tidy_df =
  df |>
  unnest(cols = c(data)) |>
  mutate(
    id = str_extract(files, "\\d+"),
    arm = str_extract(files, "con|exp")) |>
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value") |>
  mutate(
    week = as.numeric(week)) |>
  select(id, arm, week, value)

tidy_df
```

    ## # A tibble: 160 × 4
    ##    id    arm    week value
    ##    <chr> <chr> <dbl> <dbl>
    ##  1 01    con       1  0.2 
    ##  2 01    con       2 -1.31
    ##  3 01    con       3  0.66
    ##  4 01    con       4  1.96
    ##  5 01    con       5  0.23
    ##  6 01    con       6  1.09
    ##  7 01    con       7  0.05
    ##  8 01    con       8  1.94
    ##  9 02    con       1  1.13
    ## 10 02    con       2 -0.88
    ## # ℹ 150 more rows

Now, I will make a spaghetti plot showing the observations for each
subject over time by study arm/treatment group (experimental
vs. control). Most study participants started around the same values.
The control group remained mostly the same over the 8 weeks whereas the
experimental arm saw larger improvements, which indicates there was
likely not a placebo effect among the control group.

``` r
tidy_df |>
  ggplot(aes(x = week, y = value, color = id)) +
  geom_line() +
  facet_grid(. ~ arm, labeller = labeller(arm = c(con = "Control group", exp = "Experimental group"))) +
  labs(
    x = "Week",
    y = "Observation value",
    title = "Observations over time by subject ID and treatment group",
    color = "Subject ID")
```

![](p8105_hw5_smy2122_files/figure-gfm/spaghetti%20plot-1.png)<!-- -->

``` r
tidy_df |>
  mutate(
    arm = replace(arm, arm == "con", "Control group"),
    arm = replace(arm, arm == "exp", "Experimental group")) |>
  ggplot(aes(x = week, y = value, group = id, color = arm)) +
  geom_line() +
  facet_grid(. ~ arm) +
  labs(
    x = "Week",
    y = "Observation value",
    title = "Observations over time by subject ID and treatment group",
    color = "Treatment group")
```

![](p8105_hw5_smy2122_files/figure-gfm/plot-1.png)<!-- -->

## Problem 3

First, I will create a function that runs a one-sample t-test with n =
30, mu = 0, sigma = 5, and an alpha level of 0.05.

``` r
t_test_simulations = function(n = 30, mu = mu, sigma = 5) {

    data =
      tibble(
        x = rnorm(n = n, mean = mu, sd = sigma))

    t_test =
      t.test(data, mean = mu, conf.level = 1 - 0.05) |>
      broom::tidy()
   
  return(
    tibble(
      mu = mu,
      mu_hat = pull(t_test, estimate),
      p_value = pull(t_test, p.value)))
    
}

## test function

t_test_simulations(mu = 0)
```

    ## # A tibble: 1 × 3
    ##      mu mu_hat p_value
    ##   <dbl>  <dbl>   <dbl>
    ## 1     0  0.412   0.629

Next, I will run the function 5000 times with mu = 0.

``` r
output = vector("list", 5000)

for (i in 1:5000) {
  
  output[[i]] = t_test_simulations(n = 30, mu = 0, sigma = 5)
  
}

sim_results_mu_0 = bind_rows(output)

sim_results_mu_0
```

    ## # A tibble: 5,000 × 3
    ##       mu mu_hat p_value
    ##    <dbl>  <dbl>   <dbl>
    ##  1     0  0.664  0.368 
    ##  2     0  0.551  0.534 
    ##  3     0  0.567  0.487 
    ##  4     0 -1.65   0.0599
    ##  5     0  1.19   0.229 
    ##  6     0  0.334  0.738 
    ##  7     0 -1.19   0.209 
    ##  8     0  0.122  0.887 
    ##  9     0  0.684  0.472 
    ## 10     0  1.09   0.218 
    ## # ℹ 4,990 more rows

Now, I will run the function 5000 times with mu = 0:6.

``` r
output = vector("list")

for (u in 0:6) {
for (i in 1:5000) {

  output[[i]] = t_test_simulations(n = 30, mu = u, sigma = 5)
  
}
}

sim_results_all_mu = bind_rows(output)

sim_results_all_mu
```

    ## # A tibble: 5,000 × 3
    ##       mu mu_hat  p_value
    ##    <int>  <dbl>    <dbl>
    ##  1     6   7.34 2.53e-10
    ##  2     6   5.94 3.32e- 7
    ##  3     6   6.00 2.08e- 6
    ##  4     6   6.34 6.19e- 7
    ##  5     6   5.82 5.19e- 7
    ##  6     6   7.52 2.24e- 8
    ##  7     6   6.90 1.81e- 9
    ##  8     6   4.45 1.42e- 6
    ##  9     6   6.02 4.45e- 8
    ## 10     6   7.33 1.09e-10
    ## # ℹ 4,990 more rows

Next, I will make a plot showing the proportion of the number of times
the null was rejected on the y-axis and the true value of mu on the
x-axis.

``` r
sim_results_all_mu |>
  mutate(
    null = p_value<0.05) |>
  group_by(mu) |>
  summarize(
    ratio = sum(null)/n())
```

    ## # A tibble: 1 × 2
    ##      mu ratio
    ##   <int> <dbl>
    ## 1     6     1

Next, I will make a plot showing the average estimate of mu-hat on the
y-axis and the true value of mu on the x-axis.
